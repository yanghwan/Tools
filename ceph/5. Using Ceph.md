## Ceph File System 
Ceph의 분산 개체 저장소인 RADOS를 기반으로 구축된 POSIX 호환 파일 시스템   
클라이언트에 대한 데이터 I/O를 중재하는 게이트웨이 또는 브로커가 없으며, 데이터에 대한 R/W작업은 MSD 클러스터를 통해 저정됩니다. 

- architecture  
![image](https://user-images.githubusercontent.com/39255123/144989915-dd2bc872-d247-4847-be53-bb28a564df32.png)

CephFS는 2개의 RADOS pools (one for data and one for metadata.)이 필요하녀 File System를 생성 및 활성화 할수 있다.
기본적으로 하나의 FS를 만들수 있으며, 여러 파일시스템을 만들려면 "ceph fs flag set enable_multiple true" 설정이 필요하다.  



# FS
기본적으로 
.ceph fs flag set enable_multiple true

1. pool(data/metadata) create 
```bash
# ceph osd pool create cephfs_data
pool 'cephfs_data' created
# ceph osd pool create cephfs_metadata
pool 'cephfs_metadata' created

# ceph osd pool ls detail | grep cephfs_
pool 7 'cephfs_data' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 2395 flags hashpspool,creating stripe_width 0
pool 8 'cephfs_metadata' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 2398 flags hashpspool,creating stripe_width 0

```
2. filesystem create 
```bash
#$ ceph fs new <fs_name> <metadata> <data>

For Example
# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 8 and data pool 7
```

3. FS Information
```bash
#ceph fs dump  #모든 파일 시스템 설정, MDS 데몬 및 보유 순위, 대기 MDS 데몬 목록을 포함하는 지정된 epoch(기본값: 현재)에서 FSMap을 덤프
#ceph fs rm <file system name> [--yes-i-really-mean-it]  
#삭제시 1. CephFS 클러스터를 중단 ("ceph fs set <fs_name> down true ") /  down 플래그

#ceph fs get <file system name>
#ceph fs set <fs name> max_file_size <size in bytes> 
#CephFS에는 구성 가능한 최대 파일 크기가 있으며 기본적으로 1TB입니다. CephFS에 대용량 파일을 저장하려는 경우 이 제한을 더 높게 설정할 수 있습니다. 64비트 필드입니다.
#설정 max_file_size을 0으로하면 제한을 해제하지 않습니다. 단순히 빈 파일만 생성하도록 클라이언트를 제한

```
4. filesystem down
```bash
#ceph fs set <fs_name> down true    # down시 "ceph mds metadata / 
#ceph fs set <fs_name> down false    # on-line
#ceph fs fail <fs_name> #MDS Demon 중지
#ceph fs set <fs_name> joinable false #활성화 여부

For Example
# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 8 and data pool 7
```



10. 상태정보
```bash
#  ceph mds stat
myfs:1 cephfs:1 {cephfs:0=myfs.master2.sfkbas=up:creating,myfs:0=myfs.master3.yvcqtz=up:active}

# ceph mds metadata
[
    {
        "name": "myfs.master2.sfkbas",
        "addr": "[v2:192.168.178.44:6800/1624382230,v1:192.168.178.44:6801/1624382230]",
        "arch": "x86_64",
        "ceph_release": "octopus",
        "ceph_version": "ceph version 15.2.8 (bdf3eebcd22d7d0b3dd4d5501bee5bac354d5b55) octopus (stable)",
        "ceph_version_short": "15.2.8",
        "container_hostname": "master2",
        "container_image": "docker.io/ceph/ceph:v15.2.8",
        "cpu": "Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz",
        "distro": "centos",
        "distro_description": "CentOS Linux 8",
        "distro_version": "8",
        "hostname": "master2",
        "kernel_description": "#1 SMP Wed May 26 19:08:48 KST 2021",
        "kernel_version": "4.18.0-193.0.1.el8.x86_64",
        "mem_swap_kb": "8126460",
        "mem_total_kb": "7829896",
        "os": "Linux"
    },
    {
        "name": "myfs.master3.yvcqtz",
        "addr": "[v2:192.168.178.45:6800/3629801199,v1:192.168.178.45:6801/3629801199]",
        "arch": "x86_64",
        "ceph_release": "octopus",
        "ceph_version": "ceph version 15.2.8 (bdf3eebcd22d7d0b3dd4d5501bee5bac354d5b55) octopus (stable)",
        "ceph_version_short": "15.2.8",
        "container_hostname": "master3",
        "container_image": "docker.io/ceph/ceph:v15.2.8",
        "cpu": "Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz",
        "distro": "centos",
        "distro_description": "CentOS Linux 8",
        "distro_version": "8",
        "hostname": "master3",
        "kernel_description": "#1 SMP Wed May 26 19:08:48 KST 2021",
        "kernel_version": "4.18.0-193.0.1.el8.x86_64",
        "mem_swap_kb": "8126460",
        "mem_total_kb": "7829876",
        "os": "Linux"
    }
]
```

