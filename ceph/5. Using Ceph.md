```diff
- text in red
+ text in green
! text in orange
# text in gray
```

## Ceph File System 
Ceph의 분산 개체 저장소인 RADOS를 기반으로 구축된 POSIX 호환 파일 시스템   
클라이언트에 대한 데이터 I/O를 중재하는 게이트웨이 또는 브로커가 없으며, 데이터에 대한 R/W작업은 MSD 클러스터를 통해 저정됩니다. 

![image](https://user-images.githubusercontent.com/39255123/144989915-dd2bc872-d247-4847-be53-bb28a564df32.png)

* 일반적으로 설정작업은   
1. CEPH Pool >> 2. PG   >> 3. CRUSH   으로 진행  

# Pool
* Pool 기능  
Resilience (replicas ,erasure code profile)  , / Placement Groups(OSD당 PG Group 개수)  / CRUSH Rules /   Snapshots / Quotas  (Pool 최대개수)

- ceph Pool Information
```bash 
# ceph osd lspools
1 device_health_metrics
2 replicapool_hdd
3 myfs-metadata
4 myfs-data0-hdd

# ceph osd pool stats
pool device_health_metrics id 1
  nothing is going on
pool replicapool_hdd id 2
  nothing is going on
pool myfs-metadata id 3
  nothing is going on
pool myfs-data0-hdd id 4
  nothing is going on

# ceph osd pool ls  detail
pool 1 'device_health_metrics' replicated size 3 min_size 2 crush_rule 1 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 1734 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'replicapool_hdd' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 1360 lfor 0/1360/1358 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd
pool 3 'myfs-metadata' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 1506 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs
pool 4 'myfs-data0-hdd' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 1547 lfor 0/1547/1545 flags hashpspool stripe_width 0 application cephfs

# ceph osd pool autoscale-status
POOL                     SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO  BIAS  PG_NUM  NEW PG_NUM  AUTOSCALE  
device_health_metrics  121.9k                3.0        22356G  0.0000                                  1.0       1              on         
replicapool_hdd           19                 2.0        22356G  0.0000                                  1.0      32              off        
myfs-metadata          37037                 2.0        22356G  0.0000                                  4.0      32              off        
myfs-data0-hdd             0                 2.0        22356G  0.0000                                  1.0      32              off 

```

* 1. Pool Create
```bash

#ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] [replicated] [crush-rule-name] [expected-num-objects]
#ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]]   erasure [erasure-code-profile] [crush-rule-name] [expected_num_objects] [--autoscale-mode=<on,off,warn>]

# For Example
# ceph osd pool create hdd_pool_1 #추가 파라메터를 넣지 않으면 Default Setting
pool 'hdd_pool_1' created

```

* 2. Pool Delete
```bash
#ceph osd pool delete <pool-name> [<pool-name> --yes-i-really-really-mean-it]
- 
# mon_allow_pool_delete 옵션에 대해 true 설정
#ceph tell mon.\* injectargs '--mon-allow-pool-delete=false' #변경
#ceph auth ls | grep -C 5 {pool-name}
#ceph auth del {user}
```

* 3. Applcation Enabled / Disable (pool information setting)
cephfs for the Ceph Filesystem. / rbd for the Ceph Block Device / rgw for the Ceph Object Gateway 사용용도에 맞춰서 설정 필요  
```bash
#ceph health detail -f json-pretty 
#ceph osd pool application enable <poolname> <app> {--yes-i-really-mean-it}
#app는 cephfs / rbd / rgw 용도에 맞춰서 설정.

#Disable Application
# ceph osd pool application disable <poolname> <app> {--yes-i-really-mean-it}


For Examples
[root@master1 ~]# ceph osd pool ls detail | grep hdd_pool_1
pool 6 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1827 flags hashpspool stripe_width 0

# ceph osd pool application enable hdd_pool_1 cephfs --yes-i-really-mean-it
enabled application 'cephfs' on pool 'hdd_pool_1'

# ceph osd pool ls detail | grep hdd_pool_1
pool 6 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1832 flags hashpspool stripe_width 0 application cephfs

# ceph osd pool application disable hdd_pool_1 cephfs --yes-i-really-mean-it
disable application 'cephfs' on pool 'hdd_pool_1'
# ceph osd pool ls detail | grep hdd_pool_1
pool 6 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1834 flags hashpspool stripe_width 0

```
* 4. pool replicas setting 
```bash
#ceph osd pool set <poolname> size <num-replicas>
# ceph osd dump | grep 'replicated size'
```

* 5. pool value get / set
```bash
#ceph osd pool set {pool-name} {key} {value}
#ceph osd pool get {pool-name} {key}

For Examples
# ceph osd pool set hdd_pool_1 pg_num 64
set pool 6 pg_num to 64
# ceph osd pool get hdd_pool_1 pg_num
```

* 5. Default value
```bash
# ceph osd erasure-code-profile get default
k=2
m=2
plugin=jerasure
technique=reed_sol_van
```

* 6. Information
```bash
[root@master1 ~]# rados df
POOL_NAME                 USED  OBJECTS  CLONES  COPIES  MISSING_ON_PRIMARY  UNFOUND  DEGRADED  RD_OPS       RD  WR_OPS       WR  USED COMPR  UNDER COMPR
device_health_metrics  366 KiB        9       0      27                   0        0         0      74   74 KiB      84  390 KiB         0 B          0 B
hdd_pool_1                 0 B        0       0       0                   0        0         0       0      0 B       0      0 B         0 B          0 B
myfs-data0-hdd             0 B        0       0       0                   0        0         0       0      0 B       0      0 B         0 B          0 B
myfs-metadata          1.0 MiB       22       0      44                   0        0         0     125  141 KiB     116   78 KiB         0 B          0 B
replicapool_hdd        128 KiB        1       0       2                   0        0         0       0      0 B       2    2 KiB         0 B          0 B

total_objects    32
total_used       6.4 GiB
total_avail      22 TiB
total_space      22 TiB

# ceph osd pool ls detail
pool 2 'replicapool_hdd' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 1360 lfor 0/1360/1358 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd
pool 3 'myfs-metadata' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 1506 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs
pool 4 'myfs-data0-hdd' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 1547 lfor 0/1547/1545 flags hashpspool stripe_width 0 application cephfs
pool 5 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1802 flags hashpspool stripe_width 0

#ceph osd pool get {pool-name} crush_rule  #pool의 crush_rule 확인 
[root@master1 ~]# ceph osd pool get hdd_pool_1 crush_rule
crush_rule: replicated_rule
```  

# PG
Placement Group은 Ceph Client와 Ceph OSD Daemon간 loose coupling하는 역할을 합니다. 이는 Ceph OSD Daemon이 동적으로 추가/삭제 되더라도 rebalance를 동적으로 할 수 있도록 해줍니다.
```bash
# PG를 설정시 autoscale_mode를 Off로 설정을 해야 재조정이 되지 않는다.
# ceph osd pool set hdd_pool_1 pg_autoscale_mode off


               (OSD * 100)
   총 PG = ------------
               replicas 수
# OSD당 50 ~ 100 사이로 설정하며, rounded up to the nearest power of two (2의 배수로 설정)
#For Examples
# ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         21.83212  root default                               
-5          3.63869      host master1                           
 2    hdd   3.63869          osd.2         up   1.00000  1.00000
-3          7.27737      host master2                           
 0    hdd   3.63869          osd.0         up   1.00000  1.00000
 1    hdd   3.63869          osd.1         up   1.00000  1.00000
-7         10.91606      host master3                           
 3    hdd   3.63869          osd.3         up   1.00000  1.00000
 4    hdd   3.63869          osd.4         up   1.00000  1.00000
 5    hdd   3.63869          osd.5         up   1.00000  1.00000

                 (OSD(6) * 100)  
   총 PG(256)  = ------------
                 replicas 수 (3)   
# ceph osd pool set hdd_pool_1 pg_num 256
set pool 6 pg_num to 256
# ceph osd pool set hdd_pool_1 pgp_num 256
set pool 6 pgp_num to 256

# ceph osd  pool ls detail | grep hdd_pool_1
pool 6 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 256 pgp_num 256 autoscale_mode off last_change 2375 lfor 0/2371/2373 flags hashpspool stripe_width 0 application cephfs
```

# CRUSH
- an algorithm that can look up the physical location of data in Ceph, given the object name as input
- Object 이름이 주어졌을때, 물리적인 데이터 위치를 찾을수 잇는 알고리즘이며 , 데이터가 계층 구조를 기반으로 물리적인 장치에 분산되는 방식에 대한 정책을 정의한다.

* 1. crush rule 확인
```bash
#ceph osd crush rule ls
#ceph osd crush rule dump


For Examples
[root@master1 ~]# ceph osd crush rule ls
replicated_rule
hdd
[root@master1 ~]# 
[root@master1 ~]# ceph osd crush rule dump
[
    {
        "rule_id": 0,
        "rule_name": "replicated_rule",
        "ruleset": 0,
        "type": 1,
        "min_size": 1,
        "max_size": 10,
        "steps": [
            {
                "op": "take",
                "item": -1,
                "item_name": "default"
            },
            {
                "op": "chooseleaf_firstn",
                "num": 0,
                "type": "host"
            },
            {
                "op": "emit"
            }
        ]
    },
    {
        "rule_id": 1,
        "rule_name": "hdd",
        "ruleset": 1,
        "type": 1,
        "min_size": 1,
        "max_size": 10,
        "steps": [
            {
                "op": "take",
                "item": -2,
                "item_name": "default~hdd"
            },
            {
                "op": "chooseleaf_firstn",
                "num": 0,
                "type": "host"
            },
            {
                "op": "emit"
            }
        ]
    }
]
```

* 2. Rule/bucket  Create /move
```bash
#ceph osd crush rule create_simple [ruleset이름] [root] [bucket-type] [firstn|indep]
#ceph osd crush add-bucket [bucket 이름] [bucket 타입]
#ceph osd crush move [bucket 이름] [bucket 타입]=[bucket 이름] 

For Examples
#ceph osd crush rule create-simple hdd_rule default host firstn
#ceph osd crush add-bucket hdd_rule root
#ceph osd crush move osd.0 root=hdd_rule
moved item id 0 name 'osd.0' to location {root=hdd_rule} in crush map
#ceph osd crush move osd.5 root=hdd_rule
moved item id 5 name 'osd.5' to location {root=hdd_rule} in crush map
#ceph osd crush move ssd root=default  # hdd_rule bucket을 default bucket에 모두 추가할수 있다.

# ceph osd crush rule ls
hdd_rule

# ceph osd crush add-bucket hdd_rule root
added bucket hdd_rule type root to crush map

# ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-9                0  root hdd_rule                              
-1         21.83212  root default                               
-5          3.63869      host master1                           
 2    hdd   3.63869          osd.2         up   1.00000  1.00000
-3          7.27737      host master2                           
 0    hdd   3.63869          osd.0         up   1.00000  1.00000
 1    hdd   3.63869          osd.1         up   1.00000  1.00000
-7         10.91606      host master3                           
 3    hdd   3.63869          osd.3         up   1.00000  1.00000
 4    hdd   3.63869          osd.4         up   1.00000  1.00000
 5    hdd   3.63869          osd.5         up   1.00000  1.00000

#  ceph osd crush move osd.0 root=hdd_rule
moved item id 0 name 'osd.0' to location {root=hdd_rule} in crush map

#  ceph osd crush move osd.5 root=hdd_rule
moved item id 5 name 'osd.5' to location {root=hdd_rule} in crush map

# ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-9          7.27737  root hdd_rule                              
 0    hdd   3.63869      osd.0             up   1.00000  1.00000
 5    hdd   3.63869      osd.5             up   1.00000  1.00000
-1         14.55475  root default                               
-5          3.63869      host master1                           
 2    hdd   3.63869          osd.2         up   1.00000  1.00000
-3          3.63869      host master2                           
 1    hdd   3.63869          osd.1         up   1.00000  1.00000
-7          7.27737      host master3                           
 3    hdd   3.63869          osd.3         up   1.00000  1.00000
 4    hdd   3.63869          osd.4         up   1.00000  1.00000
 
#  ceph osd crush move hdd_rule root=default
moved item id -9 name 'hdd_rule' to location {root=default} in crush map
# ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME          STATUS  REWEIGHT  PRI-AFF
-1         21.83212  root default                                
-9          7.27737      root hdd_rule                           
 0    hdd   3.63869          osd.0          up   1.00000  1.00000
 5    hdd   3.63869          osd.5          up   1.00000  1.00000
-5          3.63869      host master1                            
 2    hdd   3.63869          osd.2          up   1.00000  1.00000
-3          3.63869      host master2                            
 1    hdd   3.63869          osd.1          up   1.00000  1.00000
-7          7.27737      host master3                            
 3    hdd   3.63869          osd.3          up   1.00000  1.00000
 4    hdd   3.63869          osd.4          up   1.00000  1.00000
```
