```diff
- text in red
+ text in green
! text in orange
# text in gray
```

## Ceph File System 
Ceph의 분산 개체 저장소인 RADOS를 기반으로 구축된 POSIX 호환 파일 시스템   
클라이언트에 대한 데이터 I/O를 중재하는 게이트웨이 또는 브로커가 없으며, 데이터에 대한 R/W작업은 MSD 클러스터를 통해 저정됩니다. 

- architecture  
![image](https://user-images.githubusercontent.com/39255123/144989915-dd2bc872-d247-4847-be53-bb28a564df32.png)

CephFS는 2개의 RADOS pools (one for data and one for metadata.)이 필요하녀 File System를 생성 및 활성화 할수 있다.
기본적으로 하나의 FS를 만들수 있으며, 여러 파일시스템을 만들려면 "ceph fs flag set enable_multiple true" 설정이 필요하다.  



# FS
기본적으로 
.ceph fs flag set enable_multiple true

1. pool(data/metadata) create 
```bash
# ceph osd pool create cephfs_data
pool 'cephfs_data' created
# ceph osd pool create cephfs_metadata
pool 'cephfs_metadata' created

# ceph osd pool ls detail | grep cephfs_
pool 7 'cephfs_data' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 2395 flags hashpspool,creating stripe_width 0
pool 8 'cephfs_metadata' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 2398 flags hashpspool,creating stripe_width 0

```
2. filesystem create 
```bash
#$ ceph fs new <fs_name> <metadata> <data>

For Example
# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 8 and data pool 7
```

3. FS Information
```bash
#ceph fs dump  #모든 파일 시스템 설정, MDS 데몬 및 보유 순위, 대기 MDS 데몬 목록을 포함하는 지정된 epoch(기본값: 현재)에서 FSMap을 덤프
#ceph fs rm <file system name> [--yes-i-really-mean-it]  
#삭제시 1. CephFS 클러스터를 중단 ("ceph fs set <fs_name> down true ") /  down 플래그

#ceph fs get <file system name>
#ceph fs set <fs name> max_file_size <size in bytes> 
#CephFS에는 구성 가능한 최대 파일 크기가 있으며 기본적으로 1TB입니다. CephFS에 대용량 파일을 저장하려는 경우 이 제한을 더 높게 설정할 수 있습니다. 64비트 필드입니다.
#설정 max_file_size을 0으로하면 제한을 해제하지 않습니다. 단순히 빈 파일만 생성하도록 클라이언트를 제한

```
4. filesystem down
```bash
#ceph fs set <fs_name> down true    # down시 "ceph mds metadata / 
#ceph fs set <fs_name> down false    # on-line
#ceph fs fail <fs_name> #MDS Demon 중지
#ceph fs set <fs_name> joinable false #활성화 여부

For Example
# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 8 and data pool 7
```



10. 상태정보
```bash
#  ceph mds stat
myfs:1 cephfs:1 {cephfs:0=myfs.master2.sfkbas=up:creating,myfs:0=myfs.master3.yvcqtz=up:active}

```

12. CEPH Pool >> 2. PG   >> 3. CRUSH   으로 진행  

# MDS 
CephFS 파일 시스템에는 최소 하나의 MDS가 필요하며, 자동화된 배포 도구를 사용하여 필요에 따라 필요한 MDS 서버를 시작합니다.
물리적인 메모리를 이용하여 MDS Caching를 구현을 하며, 기본 MDS 캐시 크기( MDS 캐시 구성 참조 )는 4GB이며, 상황에 따라서 많은 RAM으로 프로비저닝하는것이 좋습니다.

* mds info 
Resilience (replicas ,erasure code profile)  , / Placement Groups(OSD당 PG Group 개수)  / CRUSH Rules /   Snapshots / Quotas  (Pool 최대개수)

- ceph Pool Information
```bash 
# ceph mds metadata
[
    {
        "name": "myfs.master2.sfkbas",
        "addr": "[v2:192.168.178.44:6800/1624382230,v1:192.168.178.44:6801/1624382230]",
        "arch": "x86_64",
        "ceph_release": "octopus",
        "ceph_version": "ceph version 15.2.8 (bdf3eebcd22d7d0b3dd4d5501bee5bac354d5b55) octopus (stable)",
        "ceph_version_short": "15.2.8",
        "container_hostname": "master2",
        "container_image": "docker.io/ceph/ceph:v15.2.8",
        "cpu": "Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz",
        "distro": "centos",
        "distro_description": "CentOS Linux 8",
        "distro_version": "8",
        "hostname": "master2",
        "kernel_description": "#1 SMP Wed May 26 19:08:48 KST 2021",
        "kernel_version": "4.18.0-193.0.1.el8.x86_64",
        "mem_swap_kb": "8126460",
        "mem_total_kb": "7829896",
        "os": "Linux"
    },
    {
        "name": "myfs.master3.yvcqtz",
        "addr": "[v2:192.168.178.45:6800/3629801199,v1:192.168.178.45:6801/3629801199]",
        "arch": "x86_64",
        "ceph_release": "octopus",
        "ceph_version": "ceph version 15.2.8 (bdf3eebcd22d7d0b3dd4d5501bee5bac354d5b55) octopus (stable)",
        "ceph_version_short": "15.2.8",
        "container_hostname": "master3",
        "container_image": "docker.io/ceph/ceph:v15.2.8",
        "cpu": "Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz",
        "distro": "centos",
        "distro_description": "CentOS Linux 8",
        "distro_version": "8",
        "hostname": "master3",
        "kernel_description": "#1 SMP Wed May 26 19:08:48 KST 2021",
        "kernel_version": "4.18.0-193.0.1.el8.x86_64",
        "mem_swap_kb": "8126460",
        "mem_total_kb": "7829876",
        "os": "Linux"
    }
]
```

* 1. fs volume create
```bash
#ceph fs volume create <fs name>


# For Example
# ceph osd pool create hdd_pool_1 #추가 파라메터를 넣지 않으면 Default Setting
pool 'hdd_pool_1' created

```

* 2. Pool Delete
```bash
#ceph osd pool delete <pool-name> [<pool-name> --yes-i-really-really-mean-it]
- 
# mon_allow_pool_delete 옵션에 대해 true 설정
#ceph tell mon.\* injectargs '--mon-allow-pool-delete=false' #변경
#ceph auth ls | grep -C 5 {pool-name}
#ceph auth del {user}
```

* 3. Applcation Enabled / Disable (pool information setting)
cephfs for the Ceph Filesystem. / rbd for the Ceph Block Device / rgw for the Ceph Object Gateway 사용용도에 맞춰서 설정 필요  
```bash
#ceph health detail -f json-pretty 
#ceph osd pool application enable <poolname> <app> {--yes-i-really-mean-it}
#app는 cephfs / rbd / rgw 용도에 맞춰서 설정.

#Disable Application
# ceph osd pool application disable <poolname> <app> {--yes-i-really-mean-it}


For Examples
[root@master1 ~]# ceph osd pool ls detail | grep hdd_pool_1
pool 6 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1827 flags hashpspool stripe_width 0

# ceph osd pool application enable hdd_pool_1 cephfs --yes-i-really-mean-it
enabled application 'cephfs' on pool 'hdd_pool_1'

# ceph osd pool ls detail | grep hdd_pool_1
pool 6 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1832 flags hashpspool stripe_width 0 application cephfs

# ceph osd pool application disable hdd_pool_1 cephfs --yes-i-really-mean-it
disable application 'cephfs' on pool 'hdd_pool_1'
# ceph osd pool ls detail | grep hdd_pool_1
pool 6 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1834 flags hashpspool stripe_width 0

```
* 4. pool replicas setting 
```bash
#ceph osd pool set <poolname> size <num-replicas>
# ceph osd dump | grep 'replicated size'
```

* 5. pool value get / set
```bash
#ceph osd pool set {pool-name} {key} {value}
#ceph osd pool get {pool-name} {key}

For Examples
# ceph osd pool set hdd_pool_1 pg_num 64
set pool 6 pg_num to 64
# ceph osd pool get hdd_pool_1 pg_num
```

* 5. Default value
```bash
# ceph osd erasure-code-profile get default
k=2
m=2
plugin=jerasure
technique=reed_sol_van
```

* 6. Information
```bash
[root@master1 ~]# rados df
POOL_NAME                 USED  OBJECTS  CLONES  COPIES  MISSING_ON_PRIMARY  UNFOUND  DEGRADED  RD_OPS       RD  WR_OPS       WR  USED COMPR  UNDER COMPR
device_health_metrics  366 KiB        9       0      27                   0        0         0      74   74 KiB      84  390 KiB         0 B          0 B
hdd_pool_1                 0 B        0       0       0                   0        0         0       0      0 B       0      0 B         0 B          0 B
myfs-data0-hdd             0 B        0       0       0                   0        0         0       0      0 B       0      0 B         0 B          0 B
myfs-metadata          1.0 MiB       22       0      44                   0        0         0     125  141 KiB     116   78 KiB         0 B          0 B
replicapool_hdd        128 KiB        1       0       2                   0        0         0       0      0 B       2    2 KiB         0 B          0 B

total_objects    32
total_used       6.4 GiB
total_avail      22 TiB
total_space      22 TiB

# ceph osd pool ls detail
pool 2 'replicapool_hdd' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 1360 lfor 0/1360/1358 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd
pool 3 'myfs-metadata' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 1506 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs
pool 4 'myfs-data0-hdd' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 1547 lfor 0/1547/1545 flags hashpspool stripe_width 0 application cephfs
pool 5 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1802 flags hashpspool stripe_width 0

#ceph osd pool get {pool-name} crush_rule  #pool의 crush_rule 확인 
[root@master1 ~]# ceph osd pool get hdd_pool_1 crush_rule
crush_rule: replicated_rule
```  

# PG
Placement Group은 Ceph Client와 Ceph OSD Daemon간 loose coupling하는 역할을 합니다. 이는 Ceph OSD Daemon이 동적으로 추가/삭제 되더라도 rebalance를 동적으로 할 수 있도록 해줍니다.
```bash
# PG를 설정시 autoscale_mode를 Off로 설정을 해야 재조정이 되지 않는다.
# ceph osd pool set hdd_pool_1 pg_autoscale_mode off


               (OSD * 100)
   총 PG = ------------
               replicas 수
# OSD당 50 ~ 100 사이로 설정하며, rounded up to the nearest power of two (2의 배수로 설정)
#For Examples
# ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         21.83212  root default                               
-5          3.63869      host master1                           
 2    hdd   3.63869          osd.2         up   1.00000  1.00000
-3          7.27737      host master2                           
 0    hdd   3.63869          osd.0         up   1.00000  1.00000
 1    hdd   3.63869          osd.1         up   1.00000  1.00000
-7         10.91606      host master3                           
 3    hdd   3.63869          osd.3         up   1.00000  1.00000
 4    hdd   3.63869          osd.4         up   1.00000  1.00000
 5    hdd   3.63869          osd.5         up   1.00000  1.00000

                 (OSD(6) * 100)  
   총 PG(256)  = ------------
                 replicas 수 (3)   
# ceph osd pool set hdd_pool_1 pg_num 256
set pool 6 pg_num to 256
# ceph osd pool set hdd_pool_1 pgp_num 256
set pool 6 pgp_num to 256

# ceph osd  pool ls detail | grep hdd_pool_1
pool 6 'hdd_pool_1' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 256 pgp_num 256 autoscale_mode off last_change 2375 lfor 0/2371/2373 flags hashpspool stripe_width 0 application cephfs
```

# CRUSH
- an algorithm that can look up the physical location of data in Ceph, given the object name as input
- Object 이름이 주어졌을때, 물리적인 데이터 위치를 찾을수 잇는 알고리즘이며 , 데이터가 계층 구조를 기반으로 물리적인 장치에 분산되는 방식에 대한 정책을 정의한다.

* 1. crush rule 확인
```bash
#ceph osd crush rule ls
#ceph osd crush rule dump


For Examples
[root@master1 ~]# ceph osd crush rule ls
replicated_rule
hdd
[root@master1 ~]# 
[root@master1 ~]# ceph osd crush rule dump
[
    {
        "rule_id": 0,
        "rule_name": "replicated_rule",
        "ruleset": 0,
        "type": 1,
        "min_size": 1,
        "max_size": 10,
        "steps": [
            {
                "op": "take",
                "item": -1,
                "item_name": "default"
            },
            {
                "op": "chooseleaf_firstn",
                "num": 0,
                "type": "host"
            },
            {
                "op": "emit"
            }
        ]
    },
    {
        "rule_id": 1,
        "rule_name": "hdd",
        "ruleset": 1,
        "type": 1,
        "min_size": 1,
        "max_size": 10,
        "steps": [
            {
                "op": "take",
                "item": -2,
                "item_name": "default~hdd"
            },
            {
                "op": "chooseleaf_firstn",
                "num": 0,
                "type": "host"
            },
            {
                "op": "emit"
            }
        ]
    }
]
```

* 2. Rule/bucket  Create /move
```bash
#ceph osd crush rule create_simple [ruleset이름] [root] [bucket-type] [firstn|indep]
#ceph osd crush add-bucket [bucket 이름] [bucket 타입]
#ceph osd crush move [bucket 이름] [bucket 타입]=[bucket 이름] 

For Examples
#ceph osd crush rule create-simple hdd_rule default host firstn
#ceph osd crush add-bucket hdd_rule root
#ceph osd crush move osd.0 root=hdd_rule
moved item id 0 name 'osd.0' to location {root=hdd_rule} in crush map
#ceph osd crush move osd.5 root=hdd_rule
moved item id 5 name 'osd.5' to location {root=hdd_rule} in crush map
#ceph osd crush move ssd root=default  # hdd_rule bucket을 default bucket에 모두 추가할수 있다.

# ceph osd crush rule ls
hdd_rule

# ceph osd crush add-bucket hdd_rule root
added bucket hdd_rule type root to crush map

# ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-9                0  root hdd_rule                              
-1         21.83212  root default                               
-5          3.63869      host master1                           
 2    hdd   3.63869          osd.2         up   1.00000  1.00000
-3          7.27737      host master2                           
 0    hdd   3.63869          osd.0         up   1.00000  1.00000
 1    hdd   3.63869          osd.1         up   1.00000  1.00000
-7         10.91606      host master3                           
 3    hdd   3.63869          osd.3         up   1.00000  1.00000
 4    hdd   3.63869          osd.4         up   1.00000  1.00000
 5    hdd   3.63869          osd.5         up   1.00000  1.00000

#  ceph osd crush move osd.0 root=hdd_rule
moved item id 0 name 'osd.0' to location {root=hdd_rule} in crush map

#  ceph osd crush move osd.5 root=hdd_rule
moved item id 5 name 'osd.5' to location {root=hdd_rule} in crush map

# ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-9          7.27737  root hdd_rule                              
 0    hdd   3.63869      osd.0             up   1.00000  1.00000
 5    hdd   3.63869      osd.5             up   1.00000  1.00000
-1         14.55475  root default                               
-5          3.63869      host master1                           
 2    hdd   3.63869          osd.2         up   1.00000  1.00000
-3          3.63869      host master2                           
 1    hdd   3.63869          osd.1         up   1.00000  1.00000
-7          7.27737      host master3                           
 3    hdd   3.63869          osd.3         up   1.00000  1.00000
 4    hdd   3.63869          osd.4         up   1.00000  1.00000
 
#  ceph osd crush move hdd_rule root=default
moved item id -9 name 'hdd_rule' to location {root=default} in crush map
# ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME          STATUS  REWEIGHT  PRI-AFF
-1         21.83212  root default                                
-9          7.27737      root hdd_rule                           
 0    hdd   3.63869          osd.0          up   1.00000  1.00000
 5    hdd   3.63869          osd.5          up   1.00000  1.00000
-5          3.63869      host master1                            
 2    hdd   3.63869          osd.2          up   1.00000  1.00000
-3          3.63869      host master2                            
 1    hdd   3.63869          osd.1          up   1.00000  1.00000
-7          7.27737      host master3                            
 3    hdd   3.63869          osd.3          up   1.00000  1.00000
 4    hdd   3.63869          osd.4          up   1.00000  1.00000
```
